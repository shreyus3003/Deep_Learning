{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LeNet_pytorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMbpcihsSz7V+vusKmO2yob",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyus3003/Deep_Learning/blob/master/Home_work_4_Graded/LeNet_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhL2RtaFz29C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn import Module\n",
        "from torch import nn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets.mnist import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5IpgbLlz3q_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LeNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LeNet, self).__init__()\n",
        "    self.c1 = nn.Conv2d(1, 6, kernel_size=(5, 5))\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
        "    self.c2 = nn.Conv2d(6, 16, kernel_size=(5, 5))\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
        "    self.c3 = nn.Conv2d(16, 120, kernel_size=(5, 5))\n",
        "    self.relu3 = nn.ReLU()\n",
        "\n",
        "    self.f4 = nn.Linear(120, 84)\n",
        "    self.relu3 = nn.ReLU()\n",
        "    self.f5 = nn.Linear(84, 10)\n",
        "    self.sig = nn.LogSoftmax(dim=-1)\n",
        "      \n",
        "  def forward(self, img):\n",
        "    output = self.c1(img)\n",
        "    output = self.relu1(output)\n",
        "    output = self.pool1(output)\n",
        "\n",
        "    x = self.c2(output)\n",
        "    x = self.relu2(x)\n",
        "    x = self.pool2(x)\n",
        "\n",
        "    output = self.c2(output)\n",
        "    output = self.relu2(output)\n",
        "    output = self.pool2(output)\n",
        "\n",
        "    output += x\n",
        "\n",
        "    output = self.c3(output)\n",
        "    output = output.view(img.size(0), -1)\n",
        "    output = self.f4(output)\n",
        "    output = self.relu3(output)\n",
        "    output = self.f5(output)\n",
        "    output = self.sig(output)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zFxZkowz3tu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b5ee91fc-5b83-4cdc-ccab-42bf1233f526"
      },
      "source": [
        "data_train = MNIST('./data/mnist',download=True,transform=transforms.Compose([transforms.Resize((32, 32)),transforms.ToTensor()]))\n",
        "data_test = MNIST('./data/mnist',train=False,download=True,transform=transforms.Compose([transforms.Resize((32, 32)),transforms.ToTensor()]))\n",
        "data_train_loader = DataLoader(data_train, batch_size=256, shuffle=True, num_workers=8)\n",
        "data_test_loader = DataLoader(data_test, batch_size=1024, num_workers=8)\n",
        "\n",
        "net = LeNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=2.2e-3)\n",
        "\n",
        "def train(epoch):\n",
        "    global cur_batch_win\n",
        "    net.train()\n",
        "    loss_list, batch_list = [], []\n",
        "    for i, (images, labels) in enumerate(data_train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = net(images)\n",
        "\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        loss_list.append(loss.detach().cpu().item())\n",
        "        batch_list.append(i+1)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "          print('Train - Epoch %d, Batch: %d, Loss: %f' % (epoch, i, loss.detach().cpu().item()))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def test():\n",
        "    net.eval()\n",
        "    total_correct = 0\n",
        "    avg_loss = 0.0\n",
        "    for i, (images, labels) in enumerate(data_test_loader):\n",
        "        output = net(images)\n",
        "        avg_loss += criterion(output, labels).sum()\n",
        "        pred = output.detach().max(1)[1]\n",
        "        total_correct += pred.eq(labels.view_as(pred)).sum()\n",
        "\n",
        "    avg_loss /= len(data_test)\n",
        "    print('Test Avg. Loss: %f, Accuracy: %f' % (avg_loss.detach().cpu().item(), float(total_correct) / len(data_test)))\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "def train_and_test(epoch):\n",
        "    train(epoch)\n",
        "    test()\n",
        "\n",
        "def main():\n",
        "    for epoch in range(0, 10):\n",
        "        train_and_test(epoch)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train - Epoch 0, Batch: 0, Loss: 2.302629\n",
            "Train - Epoch 0, Batch: 100, Loss: 0.244827\n",
            "Train - Epoch 0, Batch: 200, Loss: 0.082392\n",
            "Test Avg. Loss: 0.000087, Accuracy: 0.971500\n",
            "\n",
            "\n",
            "Train - Epoch 1, Batch: 0, Loss: 0.104605\n",
            "Train - Epoch 1, Batch: 100, Loss: 0.039039\n",
            "Train - Epoch 1, Batch: 200, Loss: 0.022447\n",
            "Test Avg. Loss: 0.000059, Accuracy: 0.981400\n",
            "\n",
            "\n",
            "Train - Epoch 2, Batch: 0, Loss: 0.148065\n",
            "Train - Epoch 2, Batch: 100, Loss: 0.018632\n",
            "Train - Epoch 2, Batch: 200, Loss: 0.029119\n",
            "Test Avg. Loss: 0.000054, Accuracy: 0.983100\n",
            "\n",
            "\n",
            "Train - Epoch 3, Batch: 0, Loss: 0.047952\n",
            "Train - Epoch 3, Batch: 100, Loss: 0.057664\n",
            "Train - Epoch 3, Batch: 200, Loss: 0.032983\n",
            "Test Avg. Loss: 0.000043, Accuracy: 0.986300\n",
            "\n",
            "\n",
            "Train - Epoch 4, Batch: 0, Loss: 0.044556\n",
            "Train - Epoch 4, Batch: 100, Loss: 0.031456\n",
            "Train - Epoch 4, Batch: 200, Loss: 0.045796\n",
            "Test Avg. Loss: 0.000040, Accuracy: 0.987400\n",
            "\n",
            "\n",
            "Train - Epoch 5, Batch: 0, Loss: 0.065819\n",
            "Train - Epoch 5, Batch: 100, Loss: 0.022412\n",
            "Train - Epoch 5, Batch: 200, Loss: 0.026190\n",
            "Test Avg. Loss: 0.000038, Accuracy: 0.987700\n",
            "\n",
            "\n",
            "Train - Epoch 6, Batch: 0, Loss: 0.019182\n",
            "Train - Epoch 6, Batch: 100, Loss: 0.032597\n",
            "Train - Epoch 6, Batch: 200, Loss: 0.031779\n",
            "Test Avg. Loss: 0.000042, Accuracy: 0.985700\n",
            "\n",
            "\n",
            "Train - Epoch 7, Batch: 0, Loss: 0.027159\n",
            "Train - Epoch 7, Batch: 100, Loss: 0.033755\n",
            "Train - Epoch 7, Batch: 200, Loss: 0.020739\n",
            "Test Avg. Loss: 0.000038, Accuracy: 0.988000\n",
            "\n",
            "\n",
            "Train - Epoch 8, Batch: 0, Loss: 0.006049\n",
            "Train - Epoch 8, Batch: 100, Loss: 0.019037\n",
            "Train - Epoch 8, Batch: 200, Loss: 0.019505\n",
            "Test Avg. Loss: 0.000042, Accuracy: 0.987000\n",
            "\n",
            "\n",
            "Train - Epoch 9, Batch: 0, Loss: 0.015557\n",
            "Train - Epoch 9, Batch: 100, Loss: 0.006243\n",
            "Train - Epoch 9, Batch: 200, Loss: 0.013644\n",
            "Test Avg. Loss: 0.000038, Accuracy: 0.988300\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}